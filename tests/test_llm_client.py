# test_suite.py
import unittest
import time
import json
import re
from typing import List, Dict, Any

from llm_client import LLMClient
from config import API_KEY, BASE_URL, MODEL_NAME

class LLMTestSuite(unittest.TestCase):
    @classmethod
    def setUpClass(cls):
        """Configuraci√≥n inicial para toda la suite de pruebas"""
        cls.client = LLMClient(API_KEY, BASE_URL, MODEL_NAME)
    
    def test_connection(self):
        """Probar conexi√≥n inicial"""
        self.assertIsNotNone(self.client, "Fallo en inicializaci√≥n del cliente")

    def test_response_generation(self):
        """Probar generaci√≥n de respuesta b√°sica"""
        prompt = "Explica la teor√≠a de la relatividad en una frase"
        response = self.client.generate_response(prompt)
        
        self.assertIn('choices', response, "Respuesta no contiene campo 'choices'")
        self.assertTrue(len(response['choices']) > 0, "Respuesta vac√≠a")
        
        generated_text = response['choices'][0]['message']['content']
        self.assertIsNotNone(generated_text, "No se gener√≥ texto de respuesta")
        self.assertTrue(len(generated_text) > 0, "Respuesta generada est√° vac√≠a")

    def test_response_time(self):
        """Verificar tiempo de respuesta"""
        prompt = "Hola, ¬øc√≥mo est√°s?"
        start_time = time.time()
        response = self.client.generate_response(prompt)
        response_time = time.time() - start_time
        
        self.assertLess(response_time, 5, "Tiempo de respuesta excesivo")

    def test_context_length_limit(self):
        """Probar l√≠mite de contexto"""
        # Generar un prompt muy largo para probar el l√≠mite de contexto
        long_context = "x" * 100000  # 100,000 caracteres
        
        with self.assertRaises((ValueError, Exception), 
                                msg="No se manej√≥ correctamente el l√≠mite de contexto"):
            self.client.generate_response(long_context)

    def test_multilingual_support(self):
        """Probar soporte multilenguaje"""
        test_prompts = [
            # Diferentes idiomas y scripts
            "Hello, how are you?",  # Ingl√©s
            "Bonjour, comment √ßa va?",  # Franc√©s
            "„Åì„Çì„Å´„Å°„ÅØ„ÄÅÂÖÉÊ∞ó„Åß„Åô„ÅãÔºü",  # Japon√©s
            "¬øC√≥mo est√°s hoy?",  # Espa√±ol
            "Wie geht es dir?",  # Alem√°n
            "–ü—Ä–∏–≤–µ—Ç, –∫–∞–∫ –¥–µ–ª–∞?",  # Ruso
            "‰Ω†Â•ΩÔºå‰ªäÂ§©ÊÄé‰πàÊ†∑Ôºü",  # Chino
            "ŸÖÿ±ÿ≠ÿ®ÿßÿå ŸÉŸäŸÅ ÿ≠ÿßŸÑŸÉÿü"  # √Årabe
        ]
        
        for prompt in test_prompts:
            response = self.client.generate_response(prompt)
            generated_text = response['choices'][0]['message']['content']
            
            self.assertIsNotNone(generated_text, 
                f"No se gener√≥ respuesta para el prompt en: {prompt}")

    def test_temperature_variation(self):
        """Probar variaci√≥n de temperaturas"""
        prompt = "Cuenta una historia corta sobre un viaje"
        
        # Generar respuestas con diferentes temperaturas
        responses = {
            'low_temp': self.client.generate_response(prompt, temperature=0.1),
            'medium_temp': self.client.generate_response(prompt, temperature=0.5),
            'high_temp': self.client.generate_response(prompt, temperature=0.9)
        }
        
        # Verificar que las respuestas sean diferentes
        unique_responses = set(
            resp['choices'][0]['message']['content'] 
            for resp in responses.values()
        )
        
        # Con temperaturas m√°s altas, esperamos m√°s variaci√≥n
        self.assertTrue(len(unique_responses) > 1, 
            "Las respuestas no muestran suficiente variaci√≥n con diferentes temperaturas")

    def test_json_parsing(self):
        """Probar capacidad de generaci√≥n de JSON estructurado"""
        prompt = """
        Genera un JSON con informaci√≥n de un libro ficticio. 
        Debe contener: t√≠tulo, autor, a√±o de publicaci√≥n, g√©neros (lista), 
        y una breve sinopsis.
        """
        
        response = self.client.generate_response(prompt)
        generated_text = response['choices'][0]['message']['content']
        
        # Intentar parsear el JSON
        try:
            book_data = json.loads(generated_text)
            
            # Verificaciones de estructura
            self.assertIn('titulo', book_data)
            self.assertIn('autor', book_data)
            self.assertIn('a√±o_publicacion', book_data)
            self.assertIn('generos', book_data)
            self.assertIsInstance(book_data['generos'], list)
            self.assertIn('sinopsis', book_data)
        except json.JSONDecodeError:
            self.fail("El modelo no gener√≥ un JSON v√°lido")

    def test_code_generation(self):
        """Probar generaci√≥n de c√≥digo"""
        prompts = [
            "Escribe una funci√≥n en Python para calcular el factorial de un n√∫mero",
            "Crea un algoritmo de ordenamiento en JavaScript"
        ]
        
        for prompt in prompts:
            response = self.client.generate_response(prompt)
            generated_code = response['choices'][0]['message']['content']
            
            # Verificaciones b√°sicas de c√≥digo
            self.assertIsNotNone(generated_code, "No se gener√≥ c√≥digo")
            self.assertTrue(len(generated_code) > 0, "C√≥digo generado est√° vac√≠o")
            
            # Verificar que contenga elementos de c√≥digo
            code_indicators = [
                r'\bdef\b',  # Definici√≥n de funci√≥n en Python
                r'\bfunction\b',  # Definici√≥n de funci√≥n en JavaScript
                r'\breturn\b',  # Palabra clave de retorno
                r'\bif\b',  # Condicional
            ]
            
            code_found = any(re.search(pattern, generated_code, re.IGNORECASE) 
                              for pattern in code_indicators)
            self.assertTrue(code_found, "El c√≥digo generado no parece ser v√°lido")

    def test_reasoning_capability(self):
        """Probar capacidad de razonamiento l√≥gico"""
        reasoning_prompts = [
            "Resuelve este acertijo: Si 5 gatos atrapan 5 ratones en 5 minutos, ¬øcu√°ntos gatos se necesitar√°n para atrapar 100 ratones en 100 minutos?",
            "Un tren sale de la ciudad A a 60 km/h y otro tren sale de la ciudad B a 40 km/h en direcci√≥n opuesta. Las ciudades est√°n separadas por 500 km. ¬øEn cu√°nto tiempo se encontrar√°n?"
        ]
        
        for prompt in reasoning_prompts:
            response = self.client.generate_response(prompt)
            solution = response['choices'][0]['message']['content']
            
            self.assertIsNotNone(solution, "No se gener√≥ soluci√≥n")
            self.assertTrue(len(solution) > 10, "Soluci√≥n demasiado corta")
            
            # Verificar que incluya pasos de razonamiento
            reasoning_indicators = [
                r'\bsi\b',  # "si" en espa√±ol sugiere razonamiento
                r'\bentonces\b',  # "entonces" indica paso l√≥gico
                r'\bpor lo tanto\b',  # conclusi√≥n l√≥gica
                r'\npaso\s*[1-9]',  # indicaci√≥n de pasos
            ]
            
            reasoning_found = any(re.search(pattern, solution, re.IGNORECASE) 
                                   for pattern in reasoning_indicators)
            self.assertTrue(reasoning_found, "La soluci√≥n no muestra pasos de razonamiento")

    def test_chain_of_thought(self):
        """Evaluar razonamiento paso a paso"""
        prompt = "Explica paso a paso c√≥mo calcular la media de una lista de n√∫meros."
        response = self.client.generate_response(prompt)
        content = response['choices'][0]['message']['content']
        self.assertRegex(content, r"paso\s+\d+", msg="No se detectaron pasos en la explicaci√≥n")

    def test_markdown_table_output(self):
        """Probar si puede generar una tabla en Markdown"""
        prompt = "Genera una tabla en Markdown con tres lenguajes de programaci√≥n y su a√±o de creaci√≥n."
        response = self.client.generate_response(prompt)
        content = response['choices'][0]['message']['content']
        self.assertIn('|', content, "No se gener√≥ una tabla Markdown")
        self.assertIn('Lenguaje', content, "Faltan encabezados esperados en la tabla")

    def test_emojis_in_response(self):
        """Verificar si puede responder con emojis"""
        prompt = "Dime c√≥mo se siente alguien que gana la loter√≠a, usando emojis"
        response = self.client.generate_response(prompt)
        content = response['choices'][0]['message']['content']
        self.assertRegex(content, r"[üòÄüéâüí∞]", msg="No se detectaron emojis en la respuesta")

    def test_ambiguous_instruction(self):
        """Probar respuesta ante instrucciones ambiguas"""
        prompt = "Dime c√≥mo arreglar todo"
        response = self.client.generate_response(prompt)
        content = response['choices'][0]['message']['content']
        self.assertTrue(len(content) > 20, "Respuesta muy corta para una instrucci√≥n compleja")
        self.assertNotIn("no s√©", content.lower(), "Respuesta evasiva ante ambig√ºedad")

    def test_get_model_info(self):
        """Verificar m√©todo get_model_info"""
        model_info = self.client.get_model_info()
        self.assertIn('name', model_info)
        self.assertIn('api_base', model_info)


def run_tests():
    """Ejecutar la suite de pruebas"""
    suite = unittest.TestLoader().loadTestsFromTestCase(LLMTestSuite)
    runner = unittest.TextTestRunner(verbosity=2)
    result = runner.run(suite)
    
    # Imprimir resumen
    print(f"\nPruebas ejecutadas: {result.testsRun}")
    print(f"Errores: {len(result.errors)}")
    print(f"Fallos: {len(result.failures)}")
    
    return result.wasSuccessful()

if __name__ == '__main__':
    run_tests()